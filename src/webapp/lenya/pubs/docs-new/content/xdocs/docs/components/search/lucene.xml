<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V1.1//EN" "document-v11.dtd">

<document> 

<header> 
    <title>Lucene</title>
    <version>0.1</version> 
    <type>Documentation</type> 
    <authors>
      <person name="Michael Wechner" email="michi@apache.org"/> 
    </authors> 
</header> 
<body> 

<section>
<title>Generic Search</title>
<p>URL:</p>
<source>
<![CDATA[
/lenya/$PUB_ID/search-$AREA/lucene
]]>
</source>

<p>Indices and Excerpts:</p>
<source>
<![CDATA[
src/webapp/lenya/pubs/$PUB_ID/work/search/index/$AREA/index
src/webapp/lenya/pubs/$PUB_ID/work/search/htdocs_dump/$AREA
]]>
</source>

<p>Configuration:</p>
<source>
<![CDATA[
src/webapp/global-sitemap.xmap
]]>
</source>
</section>

<section>
<title>Customizing/Overwriting Generic Search Interface</title>
<p>XSLT:</p>
<source>
<![CDATA[
src/webapp/lenya/pubs/$PUB_ID/lenya/xslt/search/search-and-results.xsl
]]>
</source>

<p>URL:</p>
<source>
<![CDATA[
/lenya/$PUB_ID/search-$AREA/lucene
]]>
</source>
</section>


<section>
<title>Crawling a website</title>
<p>
Crawl a website by running
</p>
<source>
<![CDATA[
ant -f src/webapp/lenya/bin/crawl_and_index.xml crawl -Dcrawler.xconf=/home/username/src/cocoon-lenya/src/webapp/lenya/pubs/default/config/search/crawler-live.xconf
]]>
</source>
<p>
whereas the crawler.xconf has the following elements
</p>
<source>
<![CDATA[
<crawler>
  <user-agent>lenya</user-agent>

  <base-url href="http://cocoon.apache.org/lenya/index.html"/>
  <scope-url href="http://cocoon.apache.org/lenya/"/>

  <uri-list src="work/search/lucene/uris.txt"/>
  <htdocs-dump-dir src="work/search/lucene/htdocs_dump/cocoon.apache.org"/>

  <!-- <robots src="robots.txt" domain="cocoon.apache.org"/> -->
</crawler>
]]>
</source>
<p>
where the element robots is optional.
</p>
<p>
In case you don't have access to the server and want to disallow certain  URLs from being crawled, then
you can also define a "robots.txt" on the crawler side, e.g.
</p>
<source>
<![CDATA[
# cocoon.apache.org

User-agent: *
Disallow: /there_seems_to_be_a_bug_within_websphinx_Robot_Exclusion.html
#Disallow:

User-agent: lenya
Disallow: /do/not/crawl/this/page.html
]]>
</source>
</section>

<section>
<title>Creating an index from the command line</title>
<source>
<![CDATA[
ant -f src/webapp/lenya/bin/crawl_and_index.xml -Dlucene.xconf=/home/username/src/cocoon-lenya/src/webapp/lenya/pubs/default/config/search/lucene-live.xconf index
]]>
</source>
<p>
or
</p>
<source>
<![CDATA[
/usr/local/j2sdk1.4.2/bin/java -classpath build/lenya/webapp/WEB-INF/classes:build/lenya/webapp/WEB-INF/lib/log4j-1.2.7.jar:build/lenya/webapp/WEB-INF/lib/xercesImpl-2.5.0.jar:build/lenya/webapp/WEB-INF/lib/xml-apis.jar:build/lenya/webapp/WEB-INF/lib/lucene-1.3.jar:build/lenya/webapp/WEB-INF/lib/excalibur-io-1.1.jar org.apache.lenya.lucene.index.Index src/webapp/lenya/pubs/oscom/config/search/lucene-cmfsMatrix.xconf true
]]>
</source>
</section>

<section>
<title>Extract text from a PDF document</title>
<source>
<![CDATA[
ant -f src/webapp/lenya/bin/crawl_and_index.xml -Dhtdocs.dump.dir=/home/username/src/cocoon-lenya/src/webapp/lenya/pubs/default/work/search/lucene/htdocs_dump xpdf
]]>
</source>
<p>
Also see the targets <code>pdfbox</code> and <code>pdfadobe</code>.
</p>
</section>

</body>
</document>
